#!/bin/bash -l
# Sample script for pytorch job

## Scheduler parameters ##

#BSUB -J pytorch-3.5                # job name
#BSUB -o pytorch-3.5.%J.stdout      # optional: Have output written to specific file
#BSUB -e pytorch-3.5.%J.stderr      # optional: Have errors written to specific file
# #BSUB -q rb_highend               # optional: use highend nodes w/ Volta GPUs (default: Geforce GPUs)
#BSUB -W 2:00                       # fill in desired wallclock time [hours,]minutes (hours are optional)
#BSUB -n 1                          # min CPU cores,max CPU cores (max cores is optional)
#BSUB -M 2048                       # fill in required amount of RAM (in Mbyte)
# #BSUB -R "span[hosts=1]"          # optional: run on single host (if using more than 1 CPU cores)
# #BSUB -R "span[ptile=28]"         # optional: fill in to specify cores per node (max 28)
# #BSUB -P myProject                # optional: fill in cluster project
#BSUB -gpu "num=1:mode=exclusive_process:mps=no"

## Job parameters ##

# Anaconda virtualenv to be used
# Create before runnign the job with e.g.
# conda create -n pytorch-3.5 python=3.5 pytorch torchvision 
vEnv=pytorch-3.5 # (please change)

# Source environment (optional)
#. /fs/applications/lsf/latest/conf/profile.lsf
#. /fs/applications/modules/current/init/bash

# Load modules
module purge
module load conda/4.3.33-readonly cuda/8.0.0 cudnn/8.0_v7.0

# Activate environment
source activate $vEnv

# Run your code here (please change, this is only an example)
cat << EOT > pytorch-3.5.py
# Pytorch compute example
# from http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py
import torch
# Create two tensors.
x = torch.Tensor(5,3)
y = torch.rand(5,3)
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    print(x+y)
else:
    print("cuda not available") 

EOT
python pytorch-3.5.py
